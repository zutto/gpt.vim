command -nargs=* -range Gpt call gpt#send(<range>, <q-args>, "smart")
command -nargs=* -range CodeFast call gpt#send(<range>, <q-args>, "qwen2.5-coder:7b-instruct-q8_0")
command -nargs=* -range DeepseekSlow call gpt#send(<range>, <q-args>, "deepseek-r1:32b")
command -nargs=* -range DeepseekFast call gpt#send(<range>, <q-args>, "deepseek-r1:14b")
command -nargs=* -range GptDepeSeek call gpt#send(<range>, <q-args>, "deepseek-coder-v2:16b")
command -nargs=* -range GptSmart call gpt#send(<range>, <q-args>, "smart")
command -nargs=* -range GptComplete call gpt#complete(<range>, <q-args>, "smart")
command -nargs=* -range Gptfast call gpt#send(<range>, <q-args>, "qwen2.5-coder:3b")
command -nargs=* -range Gptslow call gpt#send(<range>, <q-args>, "qwen2.5-coder:14b")
command -nargs=* -range Gptsuperslow call gpt#send(<range>, <q-args>, "qwen2.5-coder:32b")
command -nargs=* -range Code call gpt#send(<range>, <q-args>, "pc.qwen2.5-coder:14b")
command -nargs=* -range Tiny call gpt#send(<range>, <q-args>, "pc.hermes3:3b")
command -nargs=* -range Tiny2 call gpt#send(<range>, <q-args>, "pc.deepseek-r1:1.5b")
" you can define custom model here... model handling should get some
" improvements at some point.
"command -nargs=* -range Chatgpt4 call gpt#send(<range>, <q-args>, "")
command -nargs=* ChatgptSetRole call gpt#set_role(<range>, <q-args>)
